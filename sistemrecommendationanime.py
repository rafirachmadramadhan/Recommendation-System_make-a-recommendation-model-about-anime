# -*- coding: utf-8 -*-
"""ProyekAkhir_SistemRecommendationAnime.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zBghuVWzDjt10WCfCtt9sXsdUJONE3z8

Menyiapkan akun kredensial kaggle
"""

!rm -rf ~/.kaggle && mkdir ~/.kaggle/

!mv kaggle.json ~/.kaggle/kaggle.json
!chmod 600 ~/.kaggle/kaggle.json

"""Import dataset dari kaggle"""

!kaggle datasets download -d CooperUnion/anime-recommendations-database

"""Mengesktrak dataset"""

!unzip /content/anime-recommendations-database.zip

"""Univariate Exploratory Data Analysis"""

import pandas as pd
 
Anime = pd.read_csv('/content/anime.csv')
ratings = pd.read_csv('/content/rating.csv')

"""Melihat Sebagian Data Anime"""

Anime.head()

"""Menghitung banyak anime_id tanpa nilai yang sama"""

print('Banyak data: ', len(Anime.anime_id.unique()))

"""Melihat sebagian data rating"""

ratings.head()

"""melihat rating info"""

ratings.info()

"""Melihat deskripsi rating"""

ratings.describe()

"""Menghitung banyak user_id tanpa nilai yang sama"""

print('Banyak data: ', len(ratings.user_id.unique()))

"""Data Preprocessing"""

AnimeData=ratings.merge(Anime,on='anime_id', how="left")
AnimeData.head()

"""Data Preparation General"""

AnimeData.info()

"""Menghitung banyak missing value"""

AnimeData.isnull().sum()

"""Membuang data mising value"""

AnimeDataClean = AnimeData.dropna()
AnimeDataClean.head()

"""Mengecek apakah masih ada missing value"""

AnimeDataClean.isnull().sum()

"""Data Preparation: Content Based Filtering"""

fixAnime = AnimeDataClean.sort_values('anime_id', ascending=True)
fixAnime

"""Menghitung banyak anime_id tanpa nilai yang sama"""

len(fixAnime.anime_id.unique())

"""Menyorting data anime_data"""

preparation = fixAnime
preparation.sort_values('anime_id')

"""Menghapus value yang sama pada anime_id"""

preparation = preparation.drop_duplicates('anime_id')
preparation

"""Konversi data series menjadi list"""

animeID = preparation['anime_id'].tolist()
title = preparation['name'].tolist()
Genre = preparation['genre'].tolist()

print(len(animeID))
print(len(title))
print(len(Genre))

"""Membuat dataframe baru dengan data sebelum cell ini"""

AnimeNew = pd.DataFrame({
    'AnimeID': animeID,
    'AnimeTitle': title,
    'AnimeGenre': Genre,
})
AnimeNew

"""Model Development dengan Content Based Filtering

Mengambil sample dari data AnimeNew
"""

data = AnimeNew
data.sample(5)

"""Menggunakan fungsi tfidfvectorizer() dari library sklearn"""

from sklearn.feature_extraction.text import TfidfVectorizer

tf = TfidfVectorizer()
tf.fit(data['AnimeGenre'])

"""Fit dan transformasi ke dalam bentuk matriks"""

tfidf_matrix = tf.fit_transform(data['AnimeGenre']) 
tfidf_matrix.shape

"""Menghasilkan vektor tf-idf dalam bentuk matriks"""

tfidf_matrix.todense()

"""Melihat matriks tf-idf untuk bebrapa anime dan genrenya"""

pd.DataFrame(
    tfidf_matrix.todense(), 
    columns=tf.get_feature_names(),
    index=data.AnimeID
).sample(47, axis=1).sample(10, axis=0)

"""Menghitung derajat kesamaan (similarity degree)"""

from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

"""Melihat matriks kesamaan untup bebrapa sample anime"""

cosine_sim_df = pd.DataFrame(cosine_sim, index=data['AnimeTitle'], columns=data['AnimeTitle'])
print('Shape:', cosine_sim_df.shape)
 
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Membuat fungsi anime_recommendations"""

def anime_recommendations(AnimeTitle, similarity_data=cosine_sim_df, items=data[['AnimeTitle', 'AnimeGenre']], k=5):
    index = similarity_data.loc[:,AnimeTitle].to_numpy().argpartition(
        range(-1, -k, -1))
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    closest = closest.drop(AnimeTitle, errors='ignore')
    return pd.DataFrame(closest).merge(items).head(k)

"""Melihat isi keterangan dari anime Naruto"""

data[data.AnimeTitle.eq('Naruto')]

"""Memanggil fungsi anime_recommendations untuk merekomendasikan anime yang serupa dengan Naruto"""

anime_recommendations('Naruto')

"""Data Preparation: Collaborative Filtering"""

import pandas as pd
import numpy as np 
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

df = ratings
df

"""Menyandikan (encode) fitur ‘user_id’"""

user_ids = df['user_id'].unique().tolist()
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

"""Menyandikan (encode) fitur ‘anime_id’"""

anime_ids = df['anime_id'].unique().tolist()
anime_to_anime_encoded = {x: i for i, x in enumerate(anime_ids)}
anime_encoded_to_anime = {i: x for i, x in enumerate(anime_ids)}

"""Memetakan user_id dan place_id ke dataframe yang berkaitan"""

df['user'] = df['user_id'].map(user_to_user_encoded)
df['anime'] = df['anime_id'].map(anime_to_anime_encoded)

"""Cek beberapa hal dalam data seperti jumlah user, jumlah anime, dan mengubah nilai rating menjadi float"""

import numpy as np 
num_users = len(user_to_user_encoded)
print(num_users)

num_anime = len(anime_encoded_to_anime)
print(num_anime)

df['rating'] = df['rating'].values.astype(np.float32)

min_rating = min(df['rating'])

max_rating = max(df['rating'])

print('Number of User: {}, Number of Anime: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_anime, min_rating, max_rating
))

"""Membagi Data untuk Training dan Validasi"""

df = df.sample(frac=1, random_state=42)
df

x = df[['user', 'anime']].values

y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
 
print(x, y)

"""Proses Training dengan Collaborative Filtering"""

class RecommenderNet(tf.keras.Model):
  def __init__(self, num_users, num_anime, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_anime = num_anime
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding(
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1)
    self.anime_embedding = layers.Embedding( 
        num_anime,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.anime_bias = layers.Embedding(num_anime, 1)
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) 
    user_bias = self.user_bias(inputs[:, 0])
    anime_vector = self.anime_embedding(inputs[:, 1])
    anime_bias = self.anime_bias(inputs[:, 1])
 
    dot_user_anime = tf.tensordot(user_vector, anime_vector, 2) 
 
    x = dot_user_anime + user_bias + anime_bias
    
    return tf.nn.sigmoid(x)

"""Proses compile terhadap mode"""

model = RecommenderNet(num_users, num_anime, 50) 
 
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Proses training"""

history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=128,
    epochs=5,
    verbose=1,
    validation_data=(x_val, y_val),
)

"""Visualisasi Metrik"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Mendapatkan Rekomendasi"""

animeDF = AnimeNew
df = pd.read_csv('rating.csv')

user_id = df.user_id.sample(1).iloc[0]
animewatch = df[df.user_id == user_id]

animenowatch = animeDF[~animeDF['AnimeID'].isin(animewatch.anime_id.values)]['AnimeID'] 
animenowatch = list(
    set(animenowatch)
    .intersection(set(anime_to_anime_encoded.keys()))
)
 
animenowatch = [[anime_to_anime_encoded.get(x)] for x in animenowatch]
user_encoder = user_to_user_encoded.get(user_id)
user_anime_array = np.hstack(
    ([[user_encoder]] * len(animenowatch), animenowatch)
)

ratings = model.predict(user_anime_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_anime_ids = [
    anime_encoded_to_anime.get(animenowatch[x][0]) for x in top_ratings_indices
]
 
print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Anime with high ratings from user')
print('----' * 8)
 
top_anime_user = (
    animewatch.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .anime_id.values
)
 
anime_df_rows = animeDF[animeDF['AnimeID'].isin(top_anime_user)]
for row in anime_df_rows.itertuples():
    print(row.AnimeTitle, ':', row.AnimeGenre)
 
print('----' * 8)
print('Top 10 anime recommendation')
print('----' * 8)
 
recommended_anime = animeDF[animeDF['AnimeID'].isin(recommended_anime_ids)]
for row in recommended_anime.itertuples():
    print(row.AnimeTitle, ':', row.AnimeGenre)